\documentclass{article}
\usepackage[utf8]{inputenc}

\title{CSE 383C: Numerical Linear Algebra}
\date{Fall 2016}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage[parfill]{parskip}

\setlength{\parskip}{6pt}

\DeclareMathOperator*{\argmin}{arg\,min}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]

\begin{document}

\maketitle

\section{Algorithm Complexity}

Let $A \in \mathbb{C}^{m \times n}$, $x, y, z \in \mathbb{C}^n$, $H \in \mathbb{C}^{m \times m}$, $u, v \in \mathbb{C}^{m}$.

\begin{enumerate}[label=\textbf{\roman*.}]
    \item \textbf{Inner Product $x^t y$} - $O(2n)$ time, $O(1)$ space.
    \item \textbf{Outer Product $x y^t$} - $O(n^2)$ time, $O(n^2)$ space.
    \item \textbf{Outer Product Vector $(x y^t) z = x (y^t z)$} - $O(3n)$ time, $O(n)$ space.
    \item \textbf{Dense Matrix Vector $Ax$} - $O(2mn)$ time, $O(m)$ space.
    \item \textbf{Sparse Matrix Vector $Ax$} - $O(m + n)$ time, $O(m + n)$ space.
    \item \textbf{Gram Schmidt} - $O(\frac{3}{2}mn^2)$ time.
    \item \textbf{Householder Vector $Hv = (I - 2 u u^*) v = v - 2 u u^* v$} - $O(4m)$ time.
    \item \textbf{Householder} - $O(2mn^2-\frac{2}{3}n^3)$ time.
    \item \textbf{Givens} - $O(mn^2)$ time.
    \item \textbf{SVD} - $O(2mn^2 + 11n^3)$ time.
    \item \textbf{Backsubstitution} - $O(n^2)$ time.
    \item \textbf{LU (With or without PP)} - $O(\frac{2}{3}m^3)$ time.
    \item \textbf{Cholesky} - $O(\frac{1}{3}m^3)$ time.
\end{enumerate}

\pagebreak

\section{QR Factorizations}
Given a matrix $A \in \mathbb{R}^{m \times n}$, we can write $A = QR$, where $Q$ is orthonormal and $R$ is upper triangular. This matrix factorization exists \textbf{for all} matrices.

\subsection{Reduced QR}
If $A \in \mathbb{R}^{m \times n}$, then $A = QR$ produces $Q \in \mathbb{R}^{m \times n}$ and $R \in \mathbb{R}^{n \times n}$.

The typical GS orthogonalization produces this.

\subsection{Full QR}
If $A \in \mathbb{R}^{m \times n}$, then $A = QR$ produces $Q \in \mathbb{R}^{m \times m}$ and $R \in \mathbb{R}^{m \times n}$.

A typical GS factorization loops over the columns of $A$ and orthogonalizes that column with respect to the previous columns of $A$. But if $m > n$, there are only $n$ columns of $A$ and so there are $m-n$ more orthogonal vectors that we need to form a basis for $\mathbb{R}^{m \times m}$.

This means we need $m-n$ more linearly independent vectors. Well we can just pick random vectors to orthogonalize because the probability of picking a vector that aligns exactly with a previous one (linearly dependent vector) is $0$.

Another option is just to use Householder QR, or Givens QR.

\subsection{Gram Schmidt}

Given a matrix $A \in \mathbb{R}^{m \times n}$, $A$ full rank (why? explained later), we want to form an orthogonal basis for the range of $A$.

Pick the first column of $A$, called $a_1$. We want an orthogonal basis for span of $a_1$, well we can just pick $v_1 = a_1$.

Now we are on the second column of $A$, called $a_2$. We want to now find an orthogonal vector to $v_1$. Well we can just find the projection onto $v_1$, defined as $v_1 v_1^T a_2$, and then subtract this bit off off $v_1$. So $v_2 = (I - v_1 v_1^T) a_2$.

We are now on the third column of $A$, called $a_3$. We want to find an orthogonal vector to $v_1,\ v_2$. Well we can find this by finding the component of $a_3$ that lives in $span\{v_1,\ v_2\}$, then subtracting that component from $a_3$. So $v_3 = (I - v_1 v_1^T - v_2 v_2^T) a_3$.

We continue until we have gone through every column, now we have an orthogonal basis for $Range(A)$, but this is not orthonormal. We can simply normalize each column $q_i = \frac{v_i}{|v_i|}$.

Now we have formed our matrix $Q$, and the $R$ follows. A column of $R$, say $r_j$, tells us the linear combination of $Q$ that we need to form the corresponding column $a_j$. By construction, $R$ is upper triangular.

Why does $A$ have to be full rank? If the columns of $A$ are not linearly independent, then when we try to find a orthogonal vector, we will get a $v_i = 0$, and get NaNs in our answer.

\subsection{Modified Gram Schmidt}

In CGS, we use $v_i = a_i - \sum_{j=1}^{i-1} q_j q_j^* a_i$, but if the columns of $A$ are almost linearly dependent, the inner product and subtraction operations will cause large numerical instabilities, and cause $q_i \cdot q_j \neq 0$.

Instead, we will initialize $v_i = a_i$, but then for every iteration, we do $v_i = v_i - q_j q_j^* v_i\ \forall j < i$. This makes it so that even though we have some instabilities in $R$, we focus on the orthogonality of $Q$, and we can bound $|Q^* Q - I| = O(\kappa(A) \epsilon_m)$.

\pagebreak

\section{LU Decomposition}

Let $A \in \mathbb{C}^{m \times m}$ and $A$ nonsingular. Then, $A$ admits a LU Decomposition of the form $A = LU$, where $L$ lower triangular and $U$ upper triangular, and both matrices have nonzeros along the diagonal.

Then, the solution to $LUx = b$ will be $Ux = L^{-1} b$, which has a forward substitution and a backwards substitution that take $O(2m^2)$.

If $A$ is diagonally dominant, or symmetric positive definite, then the unpivoted LU decomposition exists, and the growth factor $p = O(1)$.

\subsection{Pivoted LU Decomposition}

Even if $A$ is well conditioned, a naive LU decomposition will fail. We have to introduce pivoting at each step. \newline

\begin{theorem} Pivoted LU is backwards stable.

Let $PAQ = LU$ be the exact pivoted factorization of a non-singular matrix $A$. Let $\tilde{L},\tilde{U},\ \tilde{P},\ \tilde{Q}$ be the computed factorization on an IEEE-754 machine. 

Then,

$\tilde{L}\tilde{U} = \tilde{P}A\tilde{Q} + \delta A$; $\frac{\|\delta A\|}{\|A\|} = O(p\ \epsilon_m)$

Where $p$ is called the growth factor of $A$ and depends on the pivoting method.

\end{theorem}

\vspace{3ex}
There are a couple forms of pivoting - partial, full, and rook. In practice, partial pivoting is used, and the growth factor $p \leq 2^m$. So even though this algorithm is backwards stable, this can be potentially highly erroneous.

\subsection{Cholesky Decomposition}

If $A$ is symmetric, positive-definite, then $A = LU = R^T R$, where $R$ is upper triangular. This takes half the amount of time and space as a typical LU decomposition.

\pagebreak

\section{Least Squares}

\subsection{Underdetermined Systems}

Let $A \in \mathbb{C}^{m \times n}$, where $m < n$, and $A$ full rank. Now we an infinite number of solutions $x$ to $Ax = b$. There are a couple methods to pick the ``best" $x$. \newline

\begin{theorem} Underdetermined System Error Analysis

    If $\frac{\|\Delta A\|}{\|A\|} < \sigma_{min}$, $\frac{\|\Delta b\|}{\|b\|} < \sigma_{min}$.
    
    Then $\frac{\|\Delta x\|}{\|x\|} \leq \kappa(A) \big\{ \frac{\|\Delta A\|}{\|A\|} + \frac{\|\Delta b\|}{\|b\|} \big\}$.
\end{theorem}

\subsubsection{Regularized SVD}

This method penalizes the norm of the solution $x$ by a factor of $\beta$, known as the regularization term. We have some squared terms and $\frac{1}{2}$ terms to make differentiation easier, but it is the same minimization problem.

\begin{equation}
    \argmin_x \frac{1}{2}\|Ax - b\|_2^2 + \frac{1}{2} \beta \|x\|_2^2
\end{equation}

Using the reduced SVD, we have $A = U \Sigma V^*$, and $U \in \mathbb{C}^{m \times m}$, $\Sigma \in \mathbb{C}^{m \times m}$, and $V \in \mathbb{C}^{n \times m}$.

We substitute $x = V y$, since the solution should live in the rowspace of A, and $q = U^* b$ and now we have the following.

\begin{equation}
    \argmin_y \frac{1}{2}\|\Sigma y - q\|_2^2 + \frac{1}{2} \beta \|y\|_2^2
\end{equation}

Now, each $y_i$ term is independent, and taking the partial derivatives of $y$ and setting it to $0$, we can solve for the optimal $y$.

\begin{equation}
    \argmin_{y_i} J(y_i) = \frac{1}{2} (\sigma_i y_i - q_i)^2 + \frac{1}{2} \beta y_i^2
\end{equation}
\begin{equation}
    \frac{\partial}{\partial y_i} J(y_i) = (\sigma_i^2 y - \sigma q_i) + \beta y_i
\end{equation}
\begin{equation}
    y_i = \frac{\sigma_i q_i}{\sigma_i^2 + \beta}
\end{equation}

\subsubsection{Truncated SVD}

Truncated SVD is just a specific case of regularized SVD, with $\beta = 0$.

A full SVD decomposition of $A$ gives us $\Sigma$ with have $n-m$ zero columns, which correspond with the $n - m$ rightmost columns of $V$, where $span\{v_{n-m},\ \dots, v_m\} = Null(A)$.

Truncated SVD says let's forget about the vectors in $Null(A)$, and take $\Sigma_t$ to be the first $m$ columns of $\Sigma$, and take $V_t$ to be the first $m$ columns of $V$. We now have $\Sigma_t \in \mathbb{C}^{m \times m}$, and $V_t \in \mathbb{C}^{n \times m}$. This is the reduced SVD of $A$.

The solution to $Ax = b$ is now clearly $x = V_t \Sigma_t^{-1} U^* b$.

\subsection{Rank Deficient Systems}

Let $A \in \mathbb{C}^{m \times n}$, where $m > n$, and $rank(A) < n$.

The SVD decomposition of $A$ shows us that we have some singular values that are $0$. We can ignore the bottom $n-r$ rows of $\Sigma$ and the corresponding $U$ and $V$ vectors, and solve this with the techniques described in underdetermined systems.

\subsection{Nearly Rank Deficient Systems}

If we have $\kappa_2(A) = \frac{\sigma_{max}}{\sigma_{min}}$ very large, this tells us the spread of singular values is very large, and numerical methods will have high relative error. Using the SVD of $A$, we can easily tell $A$ is ill conditioned, and set the corresponding singular values under some threshold $\tau$ to $0$, and solve an underdetermined system.

However, SVD is not viable for large systems, and we have to use Pivoted QR, which will reveal the rank of $A$. \newline

\begin{theorem} The singular values of the block matrix produced by Column Pivoted QR $R_{k,k}$ are related to the singular values of $A$.

$\sigma_k(R_{k,k}) = O(\sigma_k(A))$

\end{theorem}

\vspace{3ex}
The $R$ matrix produced by Column Pivoted QR will have small values along the diagonal, which tells us the corresponding vectors of $Q$ that are not spanned. We can ignore these values and truncate our $Q$ to $Q_t \in \mathbb{C}^{m \times r}$, and $R$ to $R_t \in \mathbb{C}^{r \times r}$. 

Now we can solve a generic $Q_t R_t x' = b$, for $x' \in \mathbb{C}^{r}$. We fill in $n-r$ values of $x'$ to $0$ to get $\bar{x} \in \mathbb{C}^{n}$. Finally, we have to permute the rows since $A P = QR$, and acquire our final solution $x = P \bar{x}$.

\pagebreak

\section{Eigenvalue Problems}

For all of this section, let $A \in \mathbb{C}^{m \times m}$. \newline

\begin{definition}{Eigenvalue/Eigenvector}

We say $v \in \mathbb{C}^m \neq \textbf{0}$ is an eigenvector of $A$ if $Av = \lambda v$.

\end{definition}

\vspace{3ex}
\begin{definition}{Spectrum}

We say the set of all eigenvalues of $A$ is the spectrum of $A$, where $\Lambda(A) \subseteq \mathbb{C}^{m \times m}$.

\end{definition}

\subsection{Eigenvalue Decomposition}

An eigenvalue decomposition of a matrix $A$ is of the form $A = X \Lambda X^{-1}$, where $X$ is nonsingular, $\Lambda$ is diagonal. We can rewrite this to the form of $AX = X \Lambda$. Note that this decomposition is not unique, since we can simply swap corresponding eigenvalue/eigenvectors, and eigenvalues may be duplicated.

From this form, it is clear that the diagonals of $\Lambda$ are the eigenvalues and the columns of $X$ are the eigenvectors. The decomposition expresses a change of basis to ``eigenvector" coordinates. \newline

\begin{definition}{Characteristic Polynomial}

$p_A(z) = (z - \lambda_1) (z - \lambda_2) \dots (z - \lambda_m)$.

\end{definition}

\vspace{3ex}
\begin{definition}{Similarity Transformation}

If $X$ is nonsingular, then we say two matrices $A$ and $B$ are similar if $B = X A X^{-1}$.

If $A$ and $B$ are similar, then they have the same characteristic polynomial, eigenvalues, geometry and algebraic multiplicities.

\end{definition}

\vspace{3ex}
\begin{definition}{Defective Matrix}

If, for a matrix $A$, there is an eigenvalue has with greater algebraic multiplicity than its geometric multiplicity, we say that eigenvalue is defective, and the matrix $A$ is defective.

\end{definition}

\vspace{3ex}
\begin{definition}{Diagonalizability}

A matrix $A$ is nondefective if and only if it admits an eigenvalue decomposition.

\end{definition}

\vspace{3ex}
\begin{definition}{Unitary Diagonalization}

A matrix $A$ is unitary diagonalizable if there exists a unique unitary matrix $Q$ such that $A = Q \Lambda Q^*$.

This matrix decomposition exists if and only if $A$ is normal. Note that a unitary diagonalization is both an eigenvalue decomposition and a singular value decomposition.

\end{definition}

\subsection{Schur Factorization}

Any square matrix, even defective ones, admit a Schur factorization. \newline

\begin{definition}{Schur Decomposition}

$A = Q T Q^*$, where $Q$ is unitary, and $T$ is upper triangular.

Since $A$ and $R$ are similar, it is clear that the eigenvalues will appear along the diagonal.

\end{definition}

\subsection{Eigenvalue Solvers}

An intuitive method to find eigenvalues is to find the roots of $det(A - \lambda I)$, but this completely impractical. This hints towards iterative methods.

For this subsection, $A = A^T \in \mathbb{R}^{m \times m}$.

\subsubsection{Power Iteration}

Assume $A = X \Lambda X^-1$, and $\lambda_1 > \lambda_2 \geq \lambda_i$. If we take a random vector $v \in \mathbb{R}^{m}$, we can represent it as $X w$.

Then, if we apply $A^k v = A^k X w$, as $k$ goes to infinity, we have $A^k v = \lambda_1^k X_1 w_1$. After normalizing this, we have acquired our first eigenvector $x$, and we can compute the corresponding maximum eigenvalue with $\|x^T A x\|$.

\subsubsection{Inverse Power Iteration}

If $A$ is nondefective, then $A^{-1}$ has the eigenvalues $\frac{1}{\lambda_i}$, where $\{\lambda_i\}$ is the spectrum of $A$.

Since we have the following eigenvalues and $\lambda_1 \geq \dots \geq \lambda_m$, we have $\frac{1}{\lambda_m} \geq \dots \geq \frac{1}{\lambda_1}$, as the eigenvalues of $A^{-1}$. 

Now we can apply the Power Iteration to $A^-1$ to get the minimum eigenvalue, the eigenvalue closest to $0$.

\subsubsection{Shift-Invert Power Method}

Note that $A - \sigma I = X (\Lambda - \sigma I) X^{-1}$, assuming $(\Lambda - \sigma I)$ is invertible. 

This gives us that $(A - \sigma I)^{-1} = X (\Lambda - \sigma I)^{-1} X^{-1}$. Now we can apply the Inverse Power Method and that will give us the eigenvalue closest to $\sigma$ and a corresponding eigenvector.

\subsubsection{Rayleigh Quotient Iteration}

\begin{definition}{Rayleigh Quotient}

Given a matrix $A$, the Rayleigh quotient of a vector $x \in R^m$ is the scalar:

$r(x) = \frac{x^T A x}{x^T x}$

This can be interpreted as the value that most acts like an eigenvalue for a vector $x$.

\end{definition}

\vspace{3ex}
The Rayleigh Quotient Iteration then combines the Rayleigh Quotient and the Inverse Power Method to guess an eigenvector, then guess the corresponding eigenvalue, and repeats.

\pagebreak

\section{Glossary}

\begin{definition}{Normal Matrix}

We say a matrix $A$ is normal if $A A^* = A^* A$.

\end{definition}


\end{document}
