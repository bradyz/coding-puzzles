ImageNet Classification with Deep Convolutional Neural Networks
A. Krizhevsky, I. Sutskever, and G. Hinton, University of Toronto
http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

================================================================================
NOTES
================================================================================

0. ABSTRACT:
neural network - 60m params, 650k neurons, 5 convolutional, 3 fully connected
performance - 37.5% top 1, 17.0% top 5 error rates
max-pooling layers -
softmax - 1000-way
GPU implementation - speedup
dropout - regularization technique (reduce overfitting)

1. INTRODUCTION:
LabelMe - 100k+ of fully-segmented images
ImageNet - 15m high-res images with 22k categories
complexity - need to learn a lot from very little
CNNs - less connections/parameters than feedforward NN, theoretical best is worse
section 3 - improve performance, decrease training time
section 4 - address overfitting
limitations - GPU memory, training time

2. THE DATASET:
ILSVRC - 1k images for each 1k categories
error rates - top 1, top 5
image processing - downsample to 256 x 256, crop rectangular
preprocessing - subtract mean from each pixel

3. THE ARCHITECTURE:
saturating nonlinearity - much slower
nonsaturating nonlinearity - f(x) = max(0, x)
neuron output - f(x) = tanh(x), f(x) = (1 + e^-x)^-1
ReLU - rectified linear unit, much faster than tanh, no input normalization
GPU - memory limits size of network trained, split half of kernels on each gpu
kernel - neuron
columnar CNN - Ciresan et al. 
local response normalization - lateral inhibition, competition for activity
local contrast normalization - Jarrett et al.
pooling layer - summarize outputs of neighboring groups in same kernel map
pooling analogy - grid of pooling units s pixels apart, summarize z x z
typical pooling - s = z
overlapping pooling - s < z, make it more difficult to overfit
optimization problem - maximizes multinomial logistic regression (log-probability)

4. REDUCING OVERFITTING:
data augmentation - use label-preserving transformations, 224 x 224 patches
technique 1 - image translations, horizontal reflections, increases 2048x
overfitting - makes networks smaller
PCA - principal component analysis, 
technique 2 - alter intensities of RGB channels
implementation - add multiples of PCA, magnitudes ~(eigenvalues x normal random)
dropout - set output to zero of hidden neuron with probability 0.5
reasoning - do not contribute towards forward pass, not considered in backprop
learning rate - takes about twice as much time to learn

5. DETAILS OF LEARNING:
training - stochastic gradient descent
initialization - some biases to 1 to accelerate ReLU in early stages
learning rate - divide by 10 when validation error stops improving

6. RESULTS:
competitor - average two classifiers, Fisher Vectors (FV), two dense features
performance - does really well
convolution kernels learned - frequency, orientation-selective, color blobs
GPU 1 kernels - color agnostic
GPU 2 kernels - color specific
last layer vector - close vectors mean similar image
similarity comparison - euclidean distance between two 4096-d vectors, slow 
efficiency - improve by training auto-encoder to compress into short binary code

7. DISCUSSION:
results - large, deep CNN does well on strictly supervised learning
performance - degrades if a single layer is removed
improvements - larger and deeper on video where time gives more information

================================================================================
REVIEW
================================================================================

0. SUMMARY:
Large, deep CNNs under stricly supervised learning show to be extremely effective
in image recognition problems. The CNN used consisted of 8 layers, 5 convolutional,
and 3 fully connected layers that combine various techniques to prevent overfitting.
The technique is also implemented on the GPU provides speedup, which is essential
to make the training time tractible.

1. MAIN CONTRIBUTION:
The main contribution provided by the paper is to prove that deep CNNs can
outperform state of the art techniques (SIFT + Fisher Vectors) and that the CNNs
withstand the test of the current scale (millions of images), and show to be 
flexible enough and have to prevent overfitting, which is critical when the 
dataset is small compared to the number of categories.

2. STRENGTHS/WEAKNESSES:
The strengths of the technique discussed centers around their impressive results 
given that they strictly use supervised learning. Another strength was the 
use fact that there were not many hyperparameters and manual tuning involved
in their implementation. This shows their techniques effectively learning.
The weaknesses would have to be the complexity and time required for training
a deep CNN. The order is on a magnitude of days, even on a GPU implementation. 
The fact that they claim their method is throttled by GPU capabilities means
if the number of images or categories were to go up, their technique might
not survive the increase in scale.

3. INSPIRING TOPICS:
The paper has several impressive methods of combating overfitting, which is the 
key to having an effective CNN. Without the overfitting techniques, the CNN would
have less layers, leading to worse results overall. Another interesting point
was the "dropout" method used in regularization. The 50% probability of a hidden
neuron's output being 0 combined with a fully connected network intuitively 
makes sense that the CNN would learn more robust connections.

4. EXPERIMENTAL RESULTS:
The ILSVRC-2010 test set error rates are very impressive and outperform state
of the art techniques with error rates of top-1 37.5% and top-5 17.0%. 
The datasets and experiments used to gauge the methods are very appriopriate as
ILSVRC is essentially the standard for object recognition.

5. EXTENSIONS:
Dropout is an effective regularization technique but increases training time by
a factor of 2. Dropout affects all hidden neurons though and a smart selection
of which neurons to "dropout", such as the top 25% weigthed neurons, could provide
a significant speedup.

6. ADDITIONAL COMMENTS:
What was really surprising was the disparity between ILSVRC-2010 results and the 
error rates obtained in the 2009 ImageNet data set, where their method had error
rates of top-1 64.7% and top-5 40.9%. This is probably due to the increased 
number of categories (1k to 10k) and images (1m to 9m).

================================================================================
DISCUSSION POINTS
================================================================================

0. How would using more than 2 GPUs affect results? They said the two-GPU net 
takes slightly less time, but provided better significantly better results.

1. What caused the disparity between the results obtained in the ILSVRC-2010 
data set and the 2009 ImageNet data set?
